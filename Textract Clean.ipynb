{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82d370b1-cc7e-418b-8188-346df38eb6ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "client = boto3.client('textract')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "437c3f1b-0e44-492b-ac7f-36fcfef2589f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'JobId': '3fa3a6f711bdd6bf30aad6b41876a202de7e679eb3a5d255d07eecd10f3423ab',\n",
       " 'ResponseMetadata': {'RequestId': '004b4bfe-1d08-4394-aa45-28e0955db87d',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '004b4bfe-1d08-4394-aa45-28e0955db87d',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '76',\n",
       "   'date': 'Mon, 22 Jan 2024 00:19:59 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.start_document_analysis(\n",
    "    DocumentLocation={\n",
    "        'S3Object': {\n",
    "            'Bucket': 'fairstone',\n",
    "            'Name': 'docqna/CaseFund/CaseFund.pdf',     \n",
    "        }\n",
    "    },\n",
    "    FeatureTypes=[\n",
    "        'LAYOUT',\"TABLES\"\n",
    "    ],\n",
    "    ClientRequestToken='cdddssdatkl',\n",
    "    # JobTag='string',\n",
    "    # NotificationChannel={\n",
    "    #     'SNSTopicArn': 'string',\n",
    "    #     'RoleArn': 'string'\n",
    "    # },\n",
    "    OutputConfig={\n",
    "        'S3Bucket': 'fairstone',\n",
    "        # 'S3Prefix': 'string'\n",
    "    },\n",
    "    # KMSKeyId='string',\n",
    "   \n",
    ")\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7fb57e-fd4f-4c89-b31c-03640174306f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "responses1 = client.get_document_analysis(\n",
    "    JobId=response['JobId']#'aa594af8fc71e7155b18610179accad70aa74f62d12ba933ccbfe9b548912c99',\n",
    " \n",
    ")\n",
    "responses1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8bb85b20-698c-4cb9-84af-2144d2ea95aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_consecutive_runs_refactored(number_list):\n",
    "    merged_list = [number_list[i] if i == 0 or number_list[i] != number_list[i-1] + 1 else None\n",
    "                   for i in range(len(number_list))]\n",
    "\n",
    "    return [num for num in merged_list if num is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "307c1492-ab0a-45cb-bdca-5cf5882f87cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import io\n",
    "from io import BytesIO\n",
    "import sys\n",
    "from pprint import pprint\n",
    "\n",
    "\n",
    "def get_rows_columns_map(table_result, blocks_map):\n",
    "    rows = {}\n",
    "    scores = []\n",
    "    merged_cells = []\n",
    "    for relationship in table_result['Relationships']:\n",
    "        if relationship['Type'] == 'MERGED_CELL':\n",
    "            merged_cells.extend(blocks_map[relationship['Ids'][0]]['Relationships'][0]['Ids'])\n",
    "        if relationship['Type'] == 'CHILD':\n",
    "            for child_id in relationship['Ids']:\n",
    "                cell = blocks_map[child_id]\n",
    "                if cell['BlockType'] == 'CELL':\n",
    "                    row_index = cell['RowIndex']\n",
    "                    col_index = cell['ColumnIndex']\n",
    "                    if row_index not in rows:\n",
    "                        # create new row\n",
    "                        rows[row_index] = {}\n",
    "                    \n",
    "                    # get confidence score\n",
    "                    scores.append(str(cell['Confidence']))\n",
    "                        \n",
    "                    # get the text value\n",
    "                    rows[row_index][col_index] = {\"text\":get_text(cell, blocks_map),\"ids\":child_id}\n",
    "                    # rows[row_index][col_index]={}\n",
    "    return rows, scores, merged_cells\n",
    "\n",
    "\n",
    "def get_text(result, blocks_map):\n",
    "    text = ''\n",
    "    if 'Relationships' in result:\n",
    "        for relationship in result['Relationships']:\n",
    "            if relationship['Type'] == 'CHILD':\n",
    "                for child_id in relationship['Ids']:\n",
    "                    word = blocks_map[child_id]\n",
    "                    if word['BlockType'] == 'WORD':\n",
    "                        if \",\" in word['Text'] and word['Text'].replace(\",\", \"\").isnumeric():\n",
    "                            text += '\"' + word['Text'] + '\"' + ' '\n",
    "                        else:\n",
    "                            text += word['Text'] + ' '\n",
    "                    if word['BlockType'] == 'SELECTION_ELEMENT':\n",
    "                        if word['SelectionStatus'] =='SELECTED':\n",
    "                            text +=  'X '\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_table_csv_results(response, blocks_map=None):\n",
    "    # Get the text blocks\n",
    "    blocks=response['Blocks']\n",
    "    # pprint(blocks)\n",
    "    if not blocks_map:\n",
    "        blocks_map = {}\n",
    "    table_blocks = []\n",
    "    for block in blocks:\n",
    "        if not blocks_map:\n",
    "            blocks_map[block['Id']] = block\n",
    "        if block['BlockType'] == \"TABLE\":\n",
    "            table_blocks.append(block)\n",
    "\n",
    "    if len(table_blocks) <= 0:\n",
    "        return \"<b> NO Table FOUND </b>\"\n",
    "    \n",
    "    rwss=[]\n",
    "    merged_cellss=[]\n",
    "    csv_text=[]\n",
    "    csv = ''\n",
    "    for index, table in enumerate(table_blocks):\n",
    "        word, rws,merged_cells=generate_table_csv(table, blocks_map, index +1)\n",
    "        csv += word\n",
    "        csv += '\\n\\n'\n",
    "        rwss.append(rws)\n",
    "        merged_cellss.append(merged_cells)\n",
    "        csv_text.append(csv)\n",
    "        \n",
    "    return csv_text, rwss, merged_cellss\n",
    "\n",
    "def generate_table_csv(table_result, blocks_map, table_index):\n",
    "    rows, scores, merged_cells = get_rows_columns_map(table_result, blocks_map)\n",
    "    table_id = 'Table_' + str(table_index)\n",
    "    \n",
    "    # get cells.\n",
    "    csv = 'Table: {0}\\n\\n'.format(table_id)\n",
    "\n",
    "    for row_index, cols in rows.items():\n",
    "        for col_index, text in cols.items():\n",
    "            col_indices = len(cols.items())\n",
    "            csv += '{}'.format(text['text']) + \",\"\n",
    "        csv += '\\n'\n",
    "        \n",
    "    csv += '\\n\\n Confidence Scores % (Table Cell) \\n'\n",
    "    cols_count = 0\n",
    "    for score in scores:\n",
    "        cols_count += 1\n",
    "        csv += score + \",\"\n",
    "        if cols_count == col_indices:\n",
    "            csv += '\\n'\n",
    "            cols_count = 0\n",
    "\n",
    "    csv += '\\n\\n\\n'\n",
    "    return csv, rows, merged_cells \n",
    "\n",
    "def csv_creator(file_name, blocks_map=None):\n",
    "    result=get_table_csv_results(file_name, blocks_map)\n",
    "    if \"NO Table FOUND\" in result:\n",
    "        return \"\"\n",
    "    else:\n",
    "        table_csv, rows,merged_cells=result[0],result[1],result[2]\n",
    "        page_list=[] \n",
    "        table_string=''\n",
    "\n",
    "        table_list=[]\n",
    "        for items in rows:\n",
    "            table_dict={}\n",
    "            for row_index, cols in items.items():\n",
    "                table_dict[row_index]=[]\n",
    "                for col_index, text in cols.items():    \n",
    "\n",
    "                    col_indices = len(cols.items())\n",
    "                    if text['ids'] in merged_cells:\n",
    "                        table_string+=text[\"text\"]\n",
    "                        if table_dict[row_index]:\n",
    "                            table_dict[row_index].pop(-1)\n",
    "                        table_dict[row_index].append(table_string)\n",
    "                    else:            \n",
    "                        table_string += '{}'.format(text['text']) + \",\"\n",
    "                        table_dict[row_index].append(text[\"text\"])\n",
    "                \n",
    "                table_string += '\\n'\n",
    "            table_list.append(table_dict)\n",
    "            cell_table=[x for x in [blocks_map[x] for x in blocks_map if blocks_map[x]['BlockType']==\"TABLE\"] if text['ids']in x['Relationships'][0]['Ids']]\n",
    "            page=cell_table[0]['Page']\n",
    "            page_list.append(page)\n",
    "        import pandas as pd\n",
    "        ids = []\n",
    "        for items in rows:\n",
    "            for v1 in items.values():\n",
    "                for v2 in v1.values(): \n",
    "                    ids.append(v2['ids'])\n",
    "        \n",
    "        header=\"\"\n",
    "        page=\"\"\n",
    "        df_list=[]\n",
    "        header_list=[]\n",
    "  \n",
    "        # Check if the first row's index size mismatches the rest of the rows\n",
    "        for table_dict in table_list:\n",
    "\n",
    "            if len(table_dict.keys())>1:\n",
    "                if len(table_dict.get(1, [])) != len(table_dict.get(2, [])):\n",
    "                    header=table_dict.pop(1)\n",
    "                    columns = table_dict.pop(2)\n",
    "                # Extract the column names from the first row\n",
    "                else:\n",
    "                    if any(table_dict[1]):\n",
    "                        columns = table_dict.pop(1)\n",
    "                    else:\n",
    "                        columns = table_dict.pop(2)\n",
    "           \n",
    "                # Convert the dictionary to a DataFrame\n",
    "                try:\n",
    "                    df = pd.DataFrame.from_dict(table_dict, orient='index', columns=columns)\n",
    "                    for page in set([blocks_map[x][\"Page\"] for x in ids]):\n",
    "                        pages=page \n",
    "                except:\n",
    "                    df = pd.DataFrame.from_dict({}, orient='index', columns=columns)\n",
    "                    for page in set([blocks_map[x][\"Page\"] for x in ids]):\n",
    "                        pages=page \n",
    "            else:\n",
    "                columns = table_dict.pop(1)\n",
    "                df = pd.DataFrame.from_dict({}, orient='index', columns=columns)\n",
    "                for page in set([blocks_map[x][\"Page\"] for x in ids]):\n",
    "                        pages=page \n",
    "            df_list.append(df)\n",
    "            header_list.append(header) \n",
    "    \n",
    "        return df_list, header_list, page_list,rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a3e0f62-55b2-4c52-8d2c-bfa9bb76056b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "def extract_words_in_bounding_box(textract_response, holder):\n",
    "    countt=0\n",
    "    words_in_box = {}\n",
    "    page=-1\n",
    "    words=[]\n",
    "    ground_truth_lines=[]\n",
    "    ground_truth_doc={}\n",
    "    lines=[]\n",
    "    table_lines={}\n",
    "    entire_doc_lines={}\n",
    "    item_type=[]\n",
    "    for item in holder:        \n",
    "        target_page=item['Page']\n",
    "        bounding_box=item['BoundingBox']\n",
    "        layout_type=item['Type']\n",
    "        item_type.append(layout_type)\n",
    "        comptroller=1\n",
    "        orchestrator=1\n",
    "        g_orchestrator=1\n",
    "        if page!= item['Page']:\n",
    "            words=[]\n",
    "            lines=[]\n",
    "            ground_truth_lines=[]\n",
    "        page=item['Page']\n",
    "        # print(page)\n",
    "        for block in textract_response['Blocks']:\n",
    "            if  (\n",
    "                block['BlockType'] == 'LINE' and\n",
    "                'Page' in block and\n",
    "                block['Page'] == target_page and\n",
    "                'Geometry' in block \n",
    "            ):\n",
    "                box = block['Geometry']['BoundingBox']\n",
    "                # print(block['Page'])\n",
    "\n",
    "                # Check if the word's bounding box is within the specified bounding box\n",
    "                if (\n",
    "                    bounding_box['Left']-0.015*bounding_box['Left'] <= box['Left'] and\n",
    "                    bounding_box['Top']-0.015*bounding_box['Top'] <= box['Top'] and\n",
    "                    (bounding_box['Left'] + bounding_box['Width'])+0.015*(bounding_box['Left'] + bounding_box['Width']) >= box['Left'] + box['Width'] and\n",
    "                    (bounding_box['Top'] + bounding_box['Height'])+0.015*(bounding_box['Top'] + bounding_box['Height']) >= box['Top'] + box['Height']\n",
    "                ):\n",
    "                    # print(box['Left'],bounding_box['Left'])\n",
    "                    # words_in_box.append(block['Text'])\n",
    "                    lines.append(block['Text'])\n",
    "                    entire_doc_lines[block['Page']]=lines\n",
    "                    \n",
    "                    if layout_type== 'LAYOUT_TABLE' and g_orchestrator==1:\n",
    "                        ground_truth_lines.append(\"<table>\")\n",
    "                        g_orchestrator=0\n",
    "                    if len(item_type)>1 and item_type[-1]!='LAYOUT_TABLE' and item_type[-2]=='LAYOUT_TABLE' and g_orchestrator==1:\n",
    "                        ground_truth_lines.append(\"</table>\")\n",
    "                        g_orchestrator=0\n",
    "                    ground_truth_lines.append(block['Text'])\n",
    "                    ground_truth_doc[block['Page']]=ground_truth_lines\n",
    "                    \n",
    "                    \n",
    "                if layout_type !='LAYOUT_TABLE':\n",
    "                    box = block['Geometry']['BoundingBox']\n",
    "                # print(block['Page'])\n",
    "\n",
    "                # Check if the word's bounding box is within the specified bounding box\n",
    "                    if (\n",
    "                        bounding_box['Left']-0.015*bounding_box['Left'] <= box['Left'] and\n",
    "                        bounding_box['Top']-0.015*bounding_box['Top'] <= box['Top'] and\n",
    "                        (bounding_box['Left'] + bounding_box['Width'])+0.015*(bounding_box['Left'] + bounding_box['Width']) >= box['Left'] + box['Width'] and\n",
    "                        (bounding_box['Top'] + bounding_box['Height'])+0.015*(bounding_box['Top'] + bounding_box['Height']) >= box['Top'] + box['Height']\n",
    "                    ):\n",
    "                        # print(box['Left'],bounding_box['Left'])\n",
    "                        # words_in_box.append(block['Text'])\n",
    "                        words.append(block['Text'])\n",
    "                        words_in_box[block['Page']]=words\n",
    "                if layout_type =='LAYOUT_TABLE':\n",
    "                    # orchestrator=comptroller\n",
    "                    if comptroller ==1:\n",
    "                        countt+=1\n",
    "                        words.append(\"<table>\")\n",
    "                        # words.append(table)\n",
    "                        words_in_box[block['Page']]=words\n",
    "                        comptroller=0\n",
    "                    \n",
    "                    box = block['Geometry']['BoundingBox']\n",
    "                    # print(block['Page'])\n",
    "\n",
    "                    # Check if the word's bounding box is within the specified bounding box\n",
    "                    if (\n",
    "                        bounding_box['Left']-0.015*bounding_box['Left'] <= box['Left'] and\n",
    "                        bounding_box['Top']-0.015*bounding_box['Top'] <= box['Top'] and\n",
    "                        (bounding_box['Left'] + bounding_box['Width'])+0.015*(bounding_box['Left'] + bounding_box['Width']) >= box['Left'] + box['Width'] and\n",
    "                        (bounding_box['Top'] + bounding_box['Height'])+0.015*(bounding_box['Top'] + bounding_box['Height']) >= box['Top'] + box['Height']\n",
    "                    ):\n",
    "                        if block['Page'] in table_lines:\n",
    "    #                     # # print(table_lines)\n",
    "                            if orchestrator==1:\n",
    "                                if not (block['Text'] in table_lines[block['Page']] or block['Text']+\" xxxxxx\" in table_lines[block['Page']]):\n",
    "                                    table_lines[block['Page']].extend([block['Text']+\" xxxxxx\"])\n",
    "                                    # print(block['Page'],[block['Text']+\" xxxxxxORCH\"])\n",
    "                                    orchestrator=0\n",
    "                            else:\n",
    "                                # continue\n",
    "                                if not block['Text'] in table_lines[block['Page']]:\n",
    "                                    table_lines[block['Page']].extend([block['Text']])\n",
    "                        else:\n",
    "                        # continue\n",
    "                            table_lines[block['Page']]=[block['Text']+\" xxxxxx\"]\n",
    "                            # print(block['Page'],[block['Text']+\" xxxxxx\"])\n",
    "                            orchestrator=0\n",
    "\n",
    "    return words_in_box, table_lines,entire_doc_lines, ground_truth_doc#'\\n'.join(words_in_box)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1e0ca514-8287-4db8-8396-7360b95b122e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6dsuw3rJbICkKbgCyheCu4VF43ZX8MXHFd2I=\n",
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6dsu21rxbIykKblNTGXaKxG2KNkO76E4vKrc=\n",
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6dsu63rtbIikKbsPGC/uiWbGl2+3HzvUbJlU=\n",
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6dsyx1rtbJSkKbjyq897elbsDPfaOoX28SwE=\n",
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6dsy22LpbJCkKbklFQtWjJLWnzmIxR22tYw0=\n",
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6dsy72L5bJykKbrupl6UPoYVssk1zheY4fM4=\n",
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6ds2x3b1bJikKbmUkzXk1PDnlJZW1wKQ42kw=\n",
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6ds223rhbKSkKbgZGRFzVWCV8vBja8i+WQPk=\n",
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6ds27379bKCkKblM7PzLMeg1xJQp5Fr97TPg=\n",
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6ds6z2LlbICkKbp4DS0n3EsqjQfS9BwcQJPty\n",
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6ds6w3L1bICgKbp701Xo27cQpZj+l6NUyiBAG\n",
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6ds6127tbICsKbp4/tR6kG5pEWqxb2KJKL0bW\n",
      "W4dkHKDMY/+8CbPrG83hEDA1iPUg6xkZrjLIk5Pxc+iUV0b+VUNCVivn3YTCDxr7umdyJbHfnUf667qQC9otaCJDgOO1lCpWeTT6ds+z275bICoKbp70rVfbAMPo3r0q9YeriFaH\n"
     ]
    }
   ],
   "source": [
    "\n",
    "responses1 = client.get_document_analysis(\n",
    "    JobId=response['JobId'], \n",
    ")\n",
    "geom=[]\n",
    "blocks_map = {}\n",
    "doc=[]\n",
    "words_only=[]\n",
    "table_holder={}\n",
    "table_lines={}\n",
    "non_layout_table_holder={}\n",
    "ground_truth_doc={}\n",
    "next_tokens=[]\n",
    "complete_response_list=[]\n",
    "for block in responses1['Blocks']:\n",
    "    holder={}\n",
    "    if block['BlockType'] in [\n",
    "        # 'LAYOUT_FIGURE',\n",
    " # 'LAYOUT_FOOTER',\n",
    " 'LAYOUT_HEADER',\n",
    " 'LAYOUT_PAGE_NUMBER',\n",
    " 'LAYOUT_SECTION_HEADER',\n",
    " 'LAYOUT_TEXT',\n",
    " 'LAYOUT_TITLE',\n",
    "         'LAYOUT_TABLE',\n",
    "       \n",
    "                             ]:\n",
    "        holder['BoundingBox']=block['Geometry']['BoundingBox']\n",
    "        holder[\"Page\"]=block['Page']\n",
    "        holder[\"Type\"]=block['BlockType']\n",
    "        geom.append(holder)\n",
    "        # print(x)\n",
    "    \n",
    "    blocks_map[block['Id']] = block\n",
    "\n",
    "        \n",
    "\n",
    "if \"NextToken\" in responses1.keys():\n",
    "    next_token=responses1['NextToken']\n",
    "    while next_token:\n",
    "        responses1 = client.get_document_analysis(\n",
    "                    JobId=response['JobId'],\n",
    "                    # MaxResults=123,\n",
    "                    NextToken=next_token,\n",
    "                    )\n",
    "        for block in responses1['Blocks']:\n",
    "            holder={}\n",
    "            if block['BlockType'] in [\n",
    "                    # 'LAYOUT_FIGURE',\n",
    "                     # 'LAYOUT_FOOTER',\n",
    "                     'LAYOUT_HEADER',\n",
    "                     'LAYOUT_PAGE_NUMBER',\n",
    "                     'LAYOUT_SECTION_HEADER',\n",
    "                     'LAYOUT_TEXT',\n",
    "                     'LAYOUT_TITLE',\n",
    "               \n",
    "                 'LAYOUT_TABLE',]:\n",
    "                holder['BoundingBox']=block['Geometry']['BoundingBox']\n",
    "                holder[\"Page\"]=block['Page']\n",
    "                holder[\"Type\"]=block['BlockType']\n",
    "                geom.append(holder)\n",
    "            blocks_map[block['Id']] = block\n",
    "        if \"NextToken\" in responses1.keys():\n",
    "            next_token=responses1['NextToken']\n",
    "        else:\n",
    "            next_token=\"\"\n",
    "            break\n",
    "\n",
    "responses1 = client.get_document_analysis(\n",
    "    JobId=response['JobId'],\n",
    "    # MaxResults=123,\n",
    "    # NextToken=nt,\n",
    ")\n",
    "result = extract_words_in_bounding_box(responses1, geom)\n",
    "doc.append(result[0])\n",
    "table_lines.update(result[1])\n",
    "non_layout_table_holder.update(result[2])\n",
    "ground_truth_doc.update(result[-1])\n",
    "\n",
    "table_result=csv_creator(responses1,blocks_map)\n",
    "if table_result:\n",
    "    table, header,pages=table_result[0], table_result[1],table_result[2]\n",
    "    \n",
    "    for ids,page in enumerate(pages):\n",
    "        doc\n",
    "        if page in table_holder:\n",
    "            table_holder[page].extend([header[ids]]+[table[ids]])\n",
    "        else:            \n",
    "            dummy=[]\n",
    "            dummy.extend([header[ids]]+[table[ids]])\n",
    "            table_holder[int(page)]= dummy\n",
    "if \"NextToken\" in responses1.keys():\n",
    "    next_token=responses1['NextToken']           \n",
    "    while next_token:\n",
    "        responses1 = client.get_document_analysis(\n",
    "            JobId=response['JobId'],\n",
    "            # MaxResults=123,\n",
    "            NextToken=next_token,\n",
    "        )\n",
    "\n",
    "        table_result=csv_creator(responses1,blocks_map)\n",
    "        if table_result:\n",
    "            table, header,pages=table_result[0], table_result[1],table_result[2]\n",
    "\n",
    "            for ids,page in enumerate(pages):\n",
    "                if page in table_holder:\n",
    "                    table_holder[page].extend([header[ids]]+[table[ids]])\n",
    "                else:      \n",
    "                    dummy=[]\n",
    "                    dummy.extend([header[ids]]+[table[ids]])\n",
    "                    table_holder[int(page)]= dummy\n",
    "\n",
    "        # print(next_token, end='\\n')    \n",
    "        next_tokens.append(next_token)\n",
    "        result = extract_words_in_bounding_box(responses1, geom)    \n",
    "        doc.append(result[0])\n",
    "        table_lines.update(result[1])\n",
    "        non_layout_table_holder.update(result[2])   \n",
    "        ground_truth_doc.update(result[-1])      \n",
    "        if \"NextToken\" in responses1.keys():\n",
    "            next_token=responses1['NextToken']\n",
    "        else:\n",
    "            next_token=\"\"\n",
    "            break\n",
    "\n",
    "for d in doc:\n",
    "    for k, v in d.items():\n",
    "        layout_table_count=v.count('<table>')\n",
    "        if k in table_holder.keys():\n",
    "            non_layout_table_count=len(table_holder[k])/2 # Table_holder has header and tables in list \n",
    "        else:\n",
    "            non_layout_table_count=0\n",
    "        if layout_table_count == non_layout_table_count:\n",
    "            continue\n",
    "        elif non_layout_table_count==0:\n",
    "            table_index_posts=[index for index, element in enumerate(v) if element == '<table>']\n",
    "            merged_list_refactored = merge_consecutive_runs_refactored(table_index_posts)\n",
    "\n",
    "            for ids in merged_list_refactored:\n",
    "                #index of all tables in extracted text\n",
    "                dynamic_table_index_posts=[index for index, element in enumerate(v) if element == '<table>']\n",
    "                # handle consecutive tables series\n",
    "                dynamic_merged_list_refactored= merge_consecutive_runs_refactored(dynamic_table_index_posts)\n",
    "                table_index_pos=v.index(\"<table>\")  \n",
    "                if table_index_pos+1 in dynamic_table_index_posts: # if consecutive tables\n",
    "                    if len(dynamic_merged_list_refactored)>1:\n",
    "                        next_non_consecutive_item=dynamic_merged_list_refactored[dynamic_merged_list_refactored.index(table_index_pos)+1]\n",
    "                        last_consecutive_series_item=dynamic_table_index_posts[dynamic_table_index_posts.index(next_non_consecutive_item)-1]\n",
    "                    else:\n",
    "                        last_consecutive_series_item=dynamic_table_index_posts[-1]\n",
    "\n",
    "                    if dynamic_merged_list_refactored[0]!=len(v): #Check that table in text is not the last item\n",
    "                        # index for the word trailing the table, handle for multiple occurence of that word- in the doc\n",
    "                        index_stopper=[i for i in range(len(non_layout_table_holder[2]) - 1) if non_layout_table_holder[2][i] == v[last_consecutive_series_item+1] and non_layout_table_holder[2][i + 1] ==  v[last_consecutive_series_item+2]][0]\n",
    "\n",
    "                        replace_text=non_layout_table_holder[k][table_index_pos-1:non_layout_table_holder[k].index(v[last_consecutive_series_item+1],index_stopper)][1:]\n",
    "                        v[table_index_pos:last_consecutive_series_item+1]=replace_text\n",
    "                    else:\n",
    "                        replace_text=non_layout_table_holder[k][table_index_pos-1:][1:]\n",
    "                        v[table_index_pos:last_consecutive_series_item+1]=replace_text\n",
    "                else:  # No consecutive table series\n",
    "                    replace_text=non_layout_table_holder[k][non_layout_table_holder[k].index(v[table_index_pos-1]):non_layout_table_holder[k].index(v[table_index_pos+1])][1:]\n",
    "                    v[table_index_pos:table_index_pos+1]=replace_text\n",
    "        elif non_layout_table_count>layout_table_count:\n",
    "            non_layout_page_word_count=[]\n",
    "            for i in range(layout_table_count*2):\n",
    "                non_layout_page_word_count.extend(table_holder[k][i])\n",
    "            length_of_non_layout_page=len(\" \".join(non_layout_page_word_count).split())+len(\" \".join(v).split())-layout_table_count\n",
    "            length_of_layout_page=len(\" \".join(non_layout_table_holder[k]).split())\n",
    "            if length_of_layout_page>length_of_non_layout_page and (abs(length_of_layout_page-length_of_non_layout_page)/max(length_of_layout_page,length_of_non_layout_page))>0.2:\n",
    "                v.insert(v.index(\"<table>\")+1,\"<table>\")\n",
    "\n",
    "table_count_list={}\n",
    "for d in doc:\n",
    "    for k, v in d.items():\n",
    "        table_count_list[k]=v.count('<table>')\n",
    "\n",
    "\n",
    "table_count_lists = []\n",
    "for k, v in table_count_list.items():\n",
    "    if v != 0:\n",
    "        table_count_lists.extend([k]*v)\n",
    "\n",
    "doc_layout_extract={}  \n",
    "if not len(table_count_lists) or  not len([x[\"Page\"] for x in  geom if x['Type']==\"LAYOUT_TABLE\"]):\n",
    "    print(\"NO TABLES FOUND\")\n",
    "    for item in doc:\n",
    "        for k, v in item.items():\n",
    "            doc_layout_extract[k]=v\n",
    "else:\n",
    "    for d in doc:\n",
    "        for k, v in d.items():\n",
    "            if v.count(\"<table>\")>1:\n",
    "                txt=v\n",
    "                ids=0        \n",
    "                for count in range(v.count(\"<table>\")):                                \n",
    "                    header_and_table=f\"{table_holder[k][ids]}\\n<tables>{table_holder[k][ids+1].to_csv(index=False, sep='|')}</tables>\"\n",
    "                    if count+1<= table_lines[k].count(\"xxxxxx\"):\n",
    "                        header_before_table=[x for x in table_lines[k] if \"xxxxxx\" in x][count].split(\"xxxxxx\")[0]\n",
    "                        if not header_before_table in header_and_table:\n",
    "                            header_and_table=f\"{header_before_table}\\n{header_and_table}\"               \n",
    "                    table_holder_index=txt.index(\"<table>\")\n",
    "                    txt[table_holder_index]=header_and_table\n",
    "                    ids+=2\n",
    "                doc_layout_extract[k]=txt\n",
    "            elif v.count(\"<table>\")==1:\n",
    "                txt=v\n",
    "                header_and_table=f\"{table_holder[k][0]}\\n<tables>{table_holder[k][1].to_csv(index=False, sep='|')}</tables>\"\n",
    "                header_before_table=[x for x in table_lines[k] if \"xxxxxx\" in x][0].split(\"xxxxxx\")[0]\n",
    "                if not header_before_table in header_and_table:\n",
    "                    header_and_table=f\"{header_before_table}\\n{header_and_table}\"\n",
    "                table_holder_index=txt.index(\"<table>\")\n",
    "                txt[table_holder_index]=header_and_table            \n",
    "                doc_layout_extract[k]=txt\n",
    "            else:\n",
    "                doc_layout_extract[k]=v\n",
    "    for k, v in ground_truth_doc.items():\n",
    "\n",
    "        page_table_idx=[doc_layout_extract[k].index(x) for x in doc_layout_extract[k] if \"<tables>\" in x]\n",
    "        ground_truth_table_ids=[ground_truth_doc[k].index(x) for x in ground_truth_doc[k] if \"<table>\" in x]\n",
    "        if page_table_idx:\n",
    "            for table_count,table_idx in enumerate(ground_truth_table_ids):\n",
    "                page_tab=doc_layout_extract[k][page_table_idx[table_count]]\n",
    "                page_tab_to_list=page_tab.split(\"\\n\")\n",
    "                ground_truth_table_header=ground_truth_doc[k][table_idx+1]\n",
    "                page_tab_headers=page_tab_to_list[0] if page_tab_to_list[0] else page_tab_to_list[1]\n",
    "                if ground_truth_table_header:\n",
    "                    found = ground_truth_table_header in page_tab_headers #any(result_table_header in item for item in tab_headers)\n",
    "                    if found:\n",
    "                        continue\n",
    "                    else:\n",
    "                        doc_layout_extract[k][page_table_idx[table_count]]=f\"{ground_truth_table_header}\\n\"+page_tab\n",
    "                else:\n",
    "                    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bff16e-1c60-4818-b9f6-6266297b69e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary containing extracted document per page\n",
    "doc_layout_extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e2a6730-48e1-4781-8105-a235a6b68c0f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "max_words = 200\n",
    "chunks = {}\n",
    "entire_list_chunk=[]\n",
    "overlap=50\n",
    "for page, lines in doc_layout_extract.items():\n",
    "    page_chunks = []\n",
    "    current_chunk = []\n",
    "    num_words = 0   \n",
    "    start_page=0\n",
    "    for line in lines:\n",
    "                   \n",
    "            \n",
    "        if len(current_chunk)<2 and entire_list_chunk and start_page==0:    \n",
    "            line_word_count=0\n",
    "            lines_length=len(entire_list_chunk[-1])\n",
    "            for ids, rows in enumerate(entire_list_chunk[-1]):\n",
    "                last_items_up=entire_list_chunk[-1][lines_length-1-ids]\n",
    "                current_chunk.insert(0,last_items_up) \n",
    "                line_word_count+=len(re.findall(r'\\w+', last_items_up)) \n",
    "                if line_word_count+ len(re.findall(r'\\w+', entire_list_chunk[-1][max(0,lines_length-2-ids)])) >overlap: \n",
    "                    break\n",
    "            num_words=line_word_count\n",
    "            start_page=1\n",
    "            \n",
    "            \n",
    "        next_num_words = num_words + len(re.findall(r'\\w+', line))   \n",
    "        \n",
    "        if next_num_words > max_words:# and \"<table>\" not in line:\n",
    "            page_chunks.append(current_chunk)\n",
    "            entire_list_chunk.append(current_chunk)\n",
    "            current_chunk = []\n",
    "            num_words = 0\n",
    "        \n",
    "        if \"<tables>\" in line:\n",
    "            \n",
    "            header=line.split(\"<tables>\")[0]\n",
    "\n",
    "            table = line.split(\"<tables>\")[-1].split(\"</tables>\")[0]\n",
    "            df = pd.read_csv(StringIO(table), sep='|')\n",
    "            # if 'Unnamed: 0' in df.columns.to_list():\n",
    "            #     df = pd.read_csv(StringIO(table), sep='|').drop('Unnamed: 0',axis=1)\n",
    " \n",
    "                \n",
    "\n",
    "            table_chunks = []\n",
    "            curr_chunk = [df.columns.to_list()]\n",
    "            words = 0\n",
    "\n",
    "            for row in df.itertuples():\n",
    "                curr_chunk.append(row) \n",
    "                words += len(re.findall(r'\\w+', str(row)))\n",
    "                # print(words)\n",
    "\n",
    "                if words > max_words:\n",
    "                    table_chunks.append(\"\\n\".join([\"| \".join(str(x) for x in curr_chunk[0])] + [\"| \".join(str(x) for x in r) for r in curr_chunk[1:]]))\n",
    "                    # curr_chunk = [curr_chunk[0]]\n",
    "                    words = len(re.findall(r'\\w+', str(curr_chunk[-1])))\n",
    "                    tab_chunk=\"\\n\".join([\"| \".join(str(x) for x in curr_chunk[0])] + [\"| \".join(str(x) for x in r) for r in curr_chunk[1:]])\n",
    "                    current_chunk.extend([header]+[tab_chunk])\n",
    "                    page_chunks.append(current_chunk)\n",
    "                    entire_list_chunk.append([header]+table_chunks)\n",
    "                    num_words=0\n",
    "                    current_chunk=[]\n",
    "                    curr_chunk = [curr_chunk[0]]\n",
    "\n",
    "            if curr_chunk != [df.columns.to_list()]:\n",
    "                table_chunks.append(\"\\n\".join([\"| \".join(str(x) for x in curr_chunk[0])] + [\"| \".join(str(x) for x in r) for r in curr_chunk[1:]]))\n",
    "                tab_chunk=\"\\n\".join([\"| \".join(str(x) for x in curr_chunk[0])] + [\"| \".join(str(x) for x in r) for r in curr_chunk[1:]])\n",
    "                         \n",
    "                current_chunk.extend([header]+[tab_chunk])\n",
    "                page_chunks.append(current_chunk)\n",
    "                entire_list_chunk.append([header]+table_chunks)\n",
    "                num_words=0\n",
    "                current_chunk=[]\n",
    "        \n",
    "        if  \"<tables>\" not in line:\n",
    "            current_chunk.append(line)    \n",
    "            num_words += len(re.findall(r'\\w+', line))\n",
    "            # if page==14:\n",
    "            #     print(current_chunk)\n",
    "\n",
    "    if current_chunk:\n",
    "        page_chunks.append(current_chunk)\n",
    "        entire_list_chunk.append(current_chunk)\n",
    "    chunks[page] = page_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "002e7668-c93e-45c6-b988-bbbc467e7f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment5 Index already exists!\n",
      "Document indexed successfully with ID: WFKKLo0ByVMZnPenRo57\n",
      "Document indexed successfully with ID: pn-KLo0BBfj2gFS_RtWw\n",
      "Document indexed successfully with ID: WVKKLo0ByVMZnPenR45k\n",
      "Document indexed successfully with ID: p3-KLo0BBfj2gFS_SNUM\n",
      "Document indexed successfully with ID: WlKKLo0ByVMZnPenSI6a\n",
      "Document indexed successfully with ID: qH-KLo0BBfj2gFS_SNXy\n",
      "Document indexed successfully with ID: W1KKLo0ByVMZnPenSY5k\n",
      "Document indexed successfully with ID: qX-KLo0BBfj2gFS_SdXr\n",
      "Document indexed successfully with ID: XFKKLo0ByVMZnPenSo4q\n",
      "Document indexed successfully with ID: qn-KLo0BBfj2gFS_StWe\n",
      "Document indexed successfully with ID: XVKKLo0ByVMZnPenS44n\n",
      "Document indexed successfully with ID: q3-KLo0BBfj2gFS_S9WK\n",
      "Document indexed successfully with ID: XlKKLo0ByVMZnPenS47n\n",
      "Document indexed successfully with ID: rH-KLo0BBfj2gFS_TNVX\n",
      "Document indexed successfully with ID: X1KKLo0ByVMZnPenTI7L\n",
      "Document indexed successfully with ID: rX-KLo0BBfj2gFS_TdVV\n",
      "Document indexed successfully with ID: YFKKLo0ByVMZnPenTY6i\n",
      "Document indexed successfully with ID: rn-KLo0BBfj2gFS_TtUX\n",
      "Document indexed successfully with ID: YVKKLo0ByVMZnPenTo6p\n",
      "Document indexed successfully with ID: r3-KLo0BBfj2gFS_TtXk\n",
      "Document indexed successfully with ID: YlKKLo0ByVMZnPenT45R\n",
      "Document indexed successfully with ID: sH-KLo0BBfj2gFS_T9XZ\n",
      "Document indexed successfully with ID: Y1KKLo0ByVMZnPenUI4j\n",
      "Document indexed successfully with ID: sX-KLo0BBfj2gFS_UNWU\n",
      "Document indexed successfully with ID: ZFKKLo0ByVMZnPenUY4R\n",
      "Document indexed successfully with ID: sn-KLo0BBfj2gFS_UdVm\n",
      "Document indexed successfully with ID: ZVKKLo0ByVMZnPenUY7o\n",
      "Document indexed successfully with ID: s3-KLo0BBfj2gFS_UtVn\n",
      "Document indexed successfully with ID: ZlKKLo0ByVMZnPenUo7E\n",
      "Document indexed successfully with ID: tH-KLo0BBfj2gFS_U9U1\n",
      "Document indexed successfully with ID: Z1KKLo0ByVMZnPenU46y\n",
      "Document indexed successfully with ID: tX-KLo0BBfj2gFS_U9Xz\n",
      "Document indexed successfully with ID: aFKKLo0ByVMZnPenVI5i\n",
      "Document indexed successfully with ID: tn-KLo0BBfj2gFS_VNXs\n",
      "Document indexed successfully with ID: aVKKLo0ByVMZnPenVY5K\n",
      "Document indexed successfully with ID: t3-KLo0BBfj2gFS_VdW9\n",
      "Document indexed successfully with ID: alKKLo0ByVMZnPenVo4_\n",
      "Document indexed successfully with ID: uH-KLo0BBfj2gFS_VtWg\n",
      "Document indexed successfully with ID: a1KKLo0ByVMZnPenVo7a\n",
      "Document indexed successfully with ID: uX-KLo0BBfj2gFS_V9Uj\n",
      "Document indexed successfully with ID: bFKKLo0ByVMZnPenV47l\n",
      "Document indexed successfully with ID: un-KLo0BBfj2gFS_WNWV\n",
      "Document indexed successfully with ID: bVKKLo0ByVMZnPenWI7Z\n",
      "Document indexed successfully with ID: u3-KLo0BBfj2gFS_WdU-\n",
      "Document indexed successfully with ID: blKKLo0ByVMZnPenWY7L\n",
      "Document indexed successfully with ID: vH-KLo0BBfj2gFS_WtUX\n",
      "Document indexed successfully with ID: b1KKLo0ByVMZnPenWo6E\n",
      "Document indexed successfully with ID: vX-KLo0BBfj2gFS_W9UK\n",
      "Document indexed successfully with ID: cFKKLo0ByVMZnPenW45x\n",
      "Document indexed successfully with ID: vn-KLo0BBfj2gFS_W9Xh\n",
      "Document indexed successfully with ID: cVKKLo0ByVMZnPenXI5m\n",
      "Document indexed successfully with ID: v3-KLo0BBfj2gFS_XNW_\n",
      "Document indexed successfully with ID: clKKLo0ByVMZnPenXY4o\n",
      "Document indexed successfully with ID: wH-KLo0BBfj2gFS_XdWw\n",
      "Document indexed successfully with ID: c1KKLo0ByVMZnPenXo4V\n",
      "Document indexed successfully with ID: wX-KLo0BBfj2gFS_XtV7\n",
      "Document indexed successfully with ID: dFKKLo0ByVMZnPenX44C\n",
      "Document indexed successfully with ID: wn-KLo0BBfj2gFS_X9VI\n",
      "Document indexed successfully with ID: dVKKLo0ByVMZnPenX46x\n",
      "Document indexed successfully with ID: w3-KLo0BBfj2gFS_YNUv\n",
      "Document indexed successfully with ID: dlKKLo0ByVMZnPenYI58\n",
      "Document indexed successfully with ID: xH-KLo0BBfj2gFS_YNXg\n",
      "Document indexed successfully with ID: d1KKLo0ByVMZnPenYY5m\n",
      "Document indexed successfully with ID: xX-KLo0BBfj2gFS_YdWn\n",
      "Document indexed successfully with ID: eFKKLo0ByVMZnPenYo4I\n",
      "Document indexed successfully with ID: xn-KLo0BBfj2gFS_YtWP\n",
      "Document indexed successfully with ID: eVKKLo0ByVMZnPenYo7M\n",
      "Document indexed successfully with ID: x3-KLo0BBfj2gFS_Y9UI\n",
      "Document indexed successfully with ID: elKKLo0ByVMZnPenY45F\n",
      "Document indexed successfully with ID: yH-KLo0BBfj2gFS_Y9Xl\n",
      "Document indexed successfully with ID: e1KKLo0ByVMZnPenZI5_\n",
      "Document indexed successfully with ID: yX-KLo0BBfj2gFS_ZNXB\n",
      "Document indexed successfully with ID: fFKKLo0ByVMZnPenZY4L\n",
      "Document indexed successfully with ID: yn-KLo0BBfj2gFS_ZdVV\n",
      "Document indexed successfully with ID: fVKKLo0ByVMZnPenZo4m\n",
      "Document indexed successfully with ID: y3-KLo0BBfj2gFS_ZtXE\n",
      "Document indexed successfully with ID: flKKLo0ByVMZnPenZ44x\n",
      "Document indexed successfully with ID: zH-KLo0BBfj2gFS_Z9V4\n"
     ]
    }
   ],
   "source": [
    "from opensearchpy import OpenSearch, RequestsHttpConnection\n",
    "from requests_aws4auth import AWS4Auth\n",
    "\n",
    "domain_endpoint = \"search-rag2-fj5bhrqjlj7bhezstpki46uhlm.us-east-1.es.amazonaws.com\"\n",
    "service = 'es'\n",
    "credentials = boto3.Session().get_credentials()\n",
    "awsauth = AWS4Auth(credentials.access_key, credentials.secret_key, \"us-east-1\", service, session_token=credentials.token)\n",
    "os_ = OpenSearch(\n",
    "    hosts = [{'host': domain_endpoint, 'port': 443}],\n",
    "    http_auth = awsauth,\n",
    "    use_ssl = True,\n",
    "    verify_certs = True,\n",
    "    timeout=120,        \n",
    "    # http_compress = True, # enables gzip compression for request bodies\n",
    "    connection_class = RequestsHttpConnection\n",
    ")\n",
    "\n",
    "mapping = {\n",
    "  'settings': {\n",
    "    'index': {  \n",
    "      'knn': True,\n",
    "      \"knn.algo_param.ef_search\": 64,            \n",
    "    }\n",
    "      },\n",
    "\n",
    "      'mappings': {  \n",
    "        'properties': {\n",
    "          'embedding': {\n",
    "            'type': 'knn_vector', \n",
    "            'dimension': 384,\n",
    "            \"method\": {\n",
    "              \"name\": \"hnsw\",       \n",
    "              \"space_type\": \"l2\",\n",
    "              \"engine\": \"lucene\",\n",
    "              \"parameters\": {\n",
    "                 \"ef_construction\": 72,\n",
    "                 \"m\":  72\n",
    "               }\n",
    "            }\n",
    "          },\n",
    "\n",
    "          'passage_id': {\n",
    "            'type': 'keyword'\n",
    "          },\n",
    "\n",
    "          'passage': {\n",
    "            'type': 'text'\n",
    "          },\n",
    "\n",
    "          'doc_id': {\n",
    "            'type': 'keyword'\n",
    "          },\n",
    "        \n",
    "          'table': {\n",
    "            'type': 'text'\n",
    "          },\n",
    "\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "# st.write(mapping)\n",
    "domain_index = f\"experiment5\"    \n",
    "\n",
    "if not os_.indices.exists(index=domain_index):        \n",
    "    os_.indices.create(index=domain_index, body=mapping)\n",
    "    # Verify that the index has been created\n",
    "    if os_.indices.exists(index=domain_index):\n",
    "        print(f\"Index {domain_index} created successfully.\")\n",
    "    else:\n",
    "        print(f\"Failed to create index '{domain_index}'.\")\n",
    "else:\n",
    "    print(f'{domain_index} Index already exists!')\n",
    "\n",
    "i = 1\n",
    "import boto3\n",
    "SAGEMAKER=boto3.client('sagemaker-runtime')\n",
    "for page, chunkks in chunks.items(): # Iterate through dict with chunk page# and content\n",
    "    chunk_id = page # take care of multiple chunks in same page (*) is used as delimiter\n",
    "   \n",
    "    \n",
    "    for chunk in chunkks:\n",
    "        passage_chunk=\"\\n\".join(chunk)\n",
    "        payload = {'text_inputs': [passage_chunk]}\n",
    "        payload = json.dumps(payload).encode('utf-8')\n",
    "\n",
    "        response = SAGEMAKER.invoke_endpoint(EndpointName=\"jumpstart-dft-hf-textembedding-all-minilm-l6-v2-2x\", \n",
    "                                                    ContentType='application/json',  \n",
    "                                                    Body=payload)\n",
    "\n",
    "        model_predictions = json.loads(response['Body'].read())\n",
    "        embedding = model_predictions['embedding'][0]\n",
    "        table=[]\n",
    "        if page in table_holder:\n",
    "            for ids in range(0,len(table_holder[page]),2):\n",
    "                header=table_holder[page][ids]\n",
    "                tsv=table_holder[page][ids+1].to_csv(index=False,sep='|')\n",
    "                table.append(header)\n",
    "                table.append(tsv)\n",
    "            table=\"\\n\".join(table)\n",
    "                \n",
    "        document = { \n",
    "            'doc_id':\"chevron-2021\",\n",
    "            'passage_id': chunk_id,\n",
    "            'passage': passage_chunk, \n",
    "            'embedding': embedding,\n",
    "            'table':table\n",
    "        }\n",
    "        try:\n",
    "            response = os_.index(index=domain_index, body=document)\n",
    "            i += 1\n",
    "            # Check the response to see if the indexing was successful\n",
    "            if response[\"result\"] == \"created\":\n",
    "                print(f\"Document indexed successfully with ID: {response['_id']}\")\n",
    "            else:\n",
    "                print(\"Failed to index document.\")\n",
    "        except RequestError as e:\n",
    "            logging.error(f\"Error indexing document to index '{domain_index}': {e}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d82067-dd10-419b-aa52-3c1ad97e297a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response= SAGEMAKER.invoke_endpoint(EndpointName=\"jumpstart-dft-hf-textembedding-all-minilm-l6-v2-2x\", \n",
    "                                                    ContentType='application/json',  \n",
    "                                                    Body=json.dumps({'text_inputs': [\"FUNDV gross IRR?\"]}))\n",
    "model_predictions = json.loads(response['Body'].read())\n",
    "embedding = model_predictions['embedding'][0]\n",
    "query = {\n",
    "    'size': 3,\n",
    "    'query': {\n",
    "        \"knn\": {\n",
    "          \"embedding\": {\n",
    "            \"vector\": embedding,\n",
    "            \"k\": 3\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "response = os_.search(index=domain_index, body=query)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd7084f-cfc7-4e8f-9d89-dd537309fc56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d336b7e-1f5b-4659-a85c-f6ec089cc932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f0f79d-2e24-49cf-aee1-099eef5366cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
